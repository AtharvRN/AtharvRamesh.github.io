<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Edge ML Pipelines That Don’t Drop Frames · Atharv Nair</title>
  <meta name="description" content="Engineering notes on keeping multi-model ADAS stacks under latency budgets on embedded hardware." />
  <link rel="stylesheet" type="text/css" href="../style.css" />
  <link rel="shortcut icon" type="image/png" href="../assets/profile.png" />
  <link href='https://fonts.googleapis.com/css?family=Nunito' rel='stylesheet'>
</head>
<body>
  <table style="width:100%;max-width:800px;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
    <tr>
      <td style="padding:2rem 0 0.5rem 0">
        <div class="post-header">
          <div class="post-title">Edge ML Pipelines That Don’t Drop Frames</div>
          <p style="margin:0.5rem 0;color:#777777;">July 2024 · 4 minute read</p>
          <p style="margin:0.5rem 0 1.5rem 0;"><a href="../index.html">← Back to home</a> · <a href="index.html">All notes</a></p>
        </div>
      </td>
    </tr>
  </table>

  <div class="post-content">
    <p>
      At Netradyne we support multiple perception models per camera stream, running on a mix of Qualcomm SNPE and NVIDIA TensorRT targets.
      The old pipeline would occasionally drop frames whenever workloads spiked. The fix was a re-architecture that made scheduling explicit.
    </p>
    <h2>Producer–consumer scheduling</h2>
    <p>
      We replaced the monolithic loop with a producer that timestamps frames and consumers that own model batches. Each consumer has a bounded queue.
      Priority boosts route safety-critical models to the front when the queue grows.
    </p>
    <h2>Async I/O &amp; profiling</h2>
    <p>
      Camera reads and writes now run asynchronously, so GPU kernels stay fed even when storage hiccups. Extensive tracing hooks feed a Grafana dashboard, making regressions obvious during on-road tests.
    </p>
    <h2>Results</h2>
    <p>
      Zero frame drops under stress testing, a 17% throughput increase, and headroom to slot in new perception models without re-tuning budgets.
      Reach out if you’d like a sanitized version of the scheduler or priority queue implementation for your stack.
    </p>
  </div>

  <table style="width:100%;max-width:800px;border:0;border-spacing:0;border-collapse:separate;margin:1rem auto 2.5rem auto;">
    <tr>
      <td style="text-align:right;font-size:small;">
        <a href="https://github.com/chengxuxin/chengxuxin.github.io">chengxuxin template</a> → <a href="https://jonbarron.info">jonbarron.info</a>
      </td>
    </tr>
  </table>
</body>
</html>
