<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Operationalizing Process Reward Models · Atharv Nair</title>
  <meta name="description" content="Lessons from fine-tuning DreamPRM-style process reward models for Lean4 theorem proving workloads." />
  <link rel="stylesheet" type="text/css" href="../style.css" />
  <link rel="shortcut icon" type="image/png" href="../assets/profile.png" />
  <link href='https://fonts.googleapis.com/css?family=Nunito' rel='stylesheet'>
</head>
<body>
  <table style="width:100%;max-width:800px;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
    <tr>
      <td style="padding:2rem 0 0.5rem 0">
        <div class="post-header">
          <div class="post-title">Operationalizing Process Reward Models</div>
          <p style="margin:0.5rem 0;color:#777777;">September 2024 · 6 minute read</p>
          <p style="margin:0.5rem 0 1.5rem 0;"><a href="../index.html">← Back to home</a> · <a href="index.html">All notes</a></p>
        </div>
      </td>
    </tr>
  </table>

  <div class="post-content">
    <p>
      Process reward models (PRMs) are a powerful way to steer long-horizon reasoning without retraining a base language model end-to-end.
      Over the past few months I applied DreamPRM-style annotation and fine-tuning to Lean4 theorem proving workloads.
      The goal was to raise solve rates for a constrained, high-signal domain while keeping inference budgets tight.
    </p>
    <h2>Labeling strategy</h2>
    <p>
      We built a token-level labeling pipeline that scores each proof step as positive, neutral, or negative based on a lightweight symbolic checker.
      The checker catches common failure signatures—looping, vacuous steps, and contradictions—so we could bias the PRM to spot dead ends earlier.
    </p>
    <h2>Training setup</h2>
    <p>
      The PRM was fine-tuned with LoRA on top of Llama-3.2 3B. We used rank-32 adapters, a cosine scheduler, and switched to mixed precision once gradient spikes stabilized.
      Training ran on Nautilus A100s via Kubernetes jobs, with NCCL data parallelism and periodic evaluation hooks writing back to Weights &amp; Biases.
    </p>
    <h2>Inference integration</h2>
    <p>
      The PRM scores each candidate proof step and filters out low-confidence branches. We combine the PRM signal with an entropy heuristic to balance exploration.
      Result: +9% solve rate, with a 17% reduction in average tokens generated per successful proof.
    </p>
    <p>
      The accompanying repository includes manifests, logging utilities, and a reference implementation of the scorer:
      <a href="https://github.com/AtharvRN/refound_finetuning" target="_blank" rel="noopener">github.com/AtharvRN/refound_finetuning</a>.
    </p>
    <p>
      Questions, suggestions, or similar workloads? I am always happy to collaborate.
    </p>
  </div>

  <table style="width:100%;max-width:800px;border:0;border-spacing:0;border-collapse:separate;margin:1rem auto 2.5rem auto;">
    <tr>
      <td style="text-align:right;font-size:small;">
        <a href="https://github.com/chengxuxin/chengxuxin.github.io">chengxuxin template</a> → <a href="https://jonbarron.info">jonbarron.info</a>
      </td>
    </tr>
  </table>
</body>
</html>
