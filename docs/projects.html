<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Atharv Nair · Projects</title>
  <meta name="description" content="Selected projects by Atharv Nair." />
  <link rel="stylesheet" type="text/css" href="style.css" />
  <link rel="icon" href="assets/favicon.ico" sizes="any" />
  <link rel="icon" type="image/png" href="assets/favicon.png" />
  <link rel="apple-touch-icon" href="assets/apple-touch-icon.png" />
</head>

<body>
  <div class="container">
    <header class="intro">
      <div>
        <h1>Projects</h1>
        <p class="contact"><a href="index.html">Back to home</a></p>
      </div>
    </header>

    <main>
      <section class="section">
        <h2>Recent Projects</h2>
        <div class="project-list">
          <article class="project">
            <h3>StealthRL</h3>
            <p class="meta">Reinforcement learning · AI safety</p>
            <p class="links"><a href="https://github.com/suraj-ranganath/StealthRL/tree/main">GitHub</a></p>
            <ul>
              <li>
                Prototyped an RL-driven paraphrasing policy (GRPO + LoRA) tuned against detector feedback while preserving
                meaning and fluency.
              </li>
              <li>
                Added semantic/fluency constraints plus an ESL-fairness penalty to reduce regressions across writing
                styles.
              </li>
              <li>
                Reduced Fast-DetectGPT score from 0.587 to 0.458 while maintaining semantic similarity of 0.944 at the best
                checkpoint (exploratory).
              </li>
            </ul>
          </article>

          <article class="project">
            <h3>Prompt-Length Optimization via Reinforcement Learning</h3>
            <p class="meta">LLMs · Post-training</p>
            <p class="links"><a href="https://github.com/ACMCMC/prompt-length-optimization">GitHub</a></p>
            <ul>
              <li>
                Formulated discrete prompt optimization as an RL problem (AdvBench, Pythia-70M) and trained with GRPO-style
                updates to shorten prompt suffixes while preserving likelihood.
              </li>
              <li>
                Achieved 16 → 9 tokens (43.8% compression) with per-token log-likelihood improving from -1.64 to -1.12 on
                representative runs.
              </li>
              <li>
                Ran ablations on entropy/credit assignment and compared against REINFORCE and PPO baselines.
              </li>
            </ul>
          </article>

          <article class="project">
            <h3>Concept Bottleneck Models for Chest X-ray</h3>
            <p class="meta">Interpretability · Medical imaging</p>
            <ul>
              <li>
                Exploring chest X-ray interpretability using VLMs (CheXAgent/MedGemma), concept detectors (CheX-DETR), and
                CBM layers.
              </li>
              <li>
                Comparing CBM variants including LF-CBM (BiomedCLIP) and VLG-CBM (CheX), and evaluating concept
                presence/accuracy via VLM/LLM queries.
              </li>
              <li>
                Visualizing learned concepts and failure cases with Grad-CAM and saliency-style analyses.
              </li>
            </ul>
          </article>

          <article class="project">
            <h3>NYC Public Transit</h3>
            <p class="meta">Course project · Data analysis</p>
            <p class="links"><a href="https://github.com/AtharvRN/NYC_Public_Transit">GitHub</a></p>
            <ul>
              <li>
                Small course project analyzing NYC public transit data, with an emphasis on clean data processing and
                communicating results clearly.
              </li>
              <li>
                Built reproducible analysis notebooks/scripts and produced plots/visuals to summarize findings.
              </li>
            </ul>
          </article>

          <article class="project">
            <h3>Probability &amp; Statistics Project Website</h3>
            <p class="meta">Web · Simulations</p>
            <ul>
              <li>
                Built a lightweight website to present a probability and statistics project with clean writeups and
                visuals.
              </li>
              <li>
                Focused on readable explanations and simple interactive elements to communicate the analysis.
              </li>
            </ul>
          </article>

          <article class="project">
            <h3>LLM Test-Time Scaling using Process Reward Models</h3>
            <p class="meta">LLMs · Post-training</p>
            <ul>
              <li>
                Built an LLM test-time reasoning prototype for Lean4 tasks using chain-of-thought style rollouts plus
                PRM-style scoring and reranking (exploratory).
              </li>
              <li>
                Implemented high-throughput inference with vLLM/FlashAttention, KV-cache batching, and evaluation harnesses
                to measure success under fixed compute.
              </li>
              <li>
                Launched Kubernetes jobs on shared A100-80GB clusters: fp16 inference, memory-capped dataloaders,
                PVC-mounted datasets, and containerized PyTorch/PEFT (LoRA) training.
              </li>
              <li>
                Automated checkpoints and Weights &amp; Biases logging for repeatable runs.
              </li>
            </ul>
          </article>
        </div>
      </section>

      <section class="section">
        <h2>Older Projects</h2>
        <div class="project-list">
          <article class="project">
            <h3>OCT Analysis with RETFound &amp; Generative Augmentations</h3>
            <p class="meta">Biomedical vision · Generative modeling</p>
            <ul>
              <li>
                Co-authored a Bioengineering 2024 paper on RETFound-based retinal OCT feature detection.
              </li>
              <li>
                Fine-tuned a foundation model pretrained on 1.6M OCTs using 1,770 labeled B-scans (SRF/IRF/drusen/PED) and
                benchmarked single-task vs multi-task vs ResNet-50 baselines.
              </li>
              <li>
                Reached 0.75-0.80 AUC-ROC and explored data augmentation via GAN/Pix2Pix and MONAI latent diffusion models
                (exploratory).
              </li>
            </ul>
          </article>

          <article class="project">
            <h3>Far-Field Speaker Verification on Mobile Robots</h3>
            <p class="meta">IEEE SP Cup 2024 · Speaker verification</p>
            <ul>
              <li>
                1st place globally at IEEE SP Cup 2024 (ICASSP): adapted ERes2Net with targeted augmentations (RIR, MUSAN,
                speed) and robot-ready scoring (cosine + adaptive s-norm).
              </li>
              <li>
                Final leaderboard: minDCF 0.67 and EER 8.93.
              </li>
            </ul>
          </article>

          <article class="project">
            <h3>Document-Level Text Simplification</h3>
            <p class="meta">Two-stage plan-guided transformer</p>
            <p>
              Designed a plan→generate pipeline in which a RoBERTa planner labels each sentence with copy/rephrase/split/delete operations using surrounding context, then feeds the tags into SIMSUM’s summarizer→simplifier stack. Training on R‑Wiki-Auto (12k docs) with curriculum scheduling, the model delivered SARI 43.56 / D-SARI 38.52 and held up on the out-of-domain PLABA medical corpus.
            </p>
            <ul>
              <li>
                Built a sentence-level planning component that predicts edit operations using document context.
              </li>
              <li>
                Conditioned generation on the planned operations to control simplification behavior and reduce unwanted
                deletions.
              </li>
            </ul>
          </article>

          <article class="project">
            <h3>Exploring Self-Supervised Learning with DINO</h3>
            <p class="meta">Self-distillation · Representation learning</p>
            <p>
              Reimplemented DINO’s student–teacher self-distillation with momentum encoders, multi-crop augmentations, and moving-average diagnostics on Imagenette. The distilled backbone exceeded supervised ResNet/Vision Transformer baselines by 12–20% top-1 accuracy, and its frozen features transfer cleanly to CIFAR-10/100 classification and Pascal VOC segmentation.
            </p>
            <ul>
              <li>
                Implemented the student-teacher training loop and stability diagnostics (EMA teacher, centering, temperature
                schedules).
              </li>
              <li>
                Evaluated representation quality via frozen-backbone transfer to downstream tasks.
              </li>
            </ul>
          </article>

          <article class="project">
            <h3>Deep Learning for OFDM Channel Estimation</h3>
            <p class="meta">Wireless communication · Model compression</p>
            <p>
              Modeled a 64-subcarrier, 16-QAM OFDM link end-to-end—pilot insertion ((3+3j) comb pattern), channel simulation, and demapper—and benchmarked classical LS/MMSE estimators against a skip-connected CNN that outputs 64×2 complex taps. The learned model closes much of the MMSE gap at low SNR while significantly outperforming LS, all within a lightweight PyTorch training loop.
            </p>
            <ul>
              <li>
                Built an end-to-end simulation pipeline to generate training and evaluation data under controlled channel
                conditions.
              </li>
              <li>
                Compared learned estimators against classical baselines across SNR regimes.
              </li>
            </ul>
          </article>

          <article class="project">
            <h3>Comprehensive Review of Image Denoising</h3>
            <p class="meta">Classical + deep pipelines</p>
            <p>
              Benchmarked wavelet, NLM, BM3D, and WNNM pipelines against autoencoder, DnCNN, RIDNet, CBDNet, and PRIDNet implementations on BSD400/CBSD68 (noise15 &amp; noise25). Architectural tweaks—LeakyReLU activations, dropout, and cascaded enhancement attention—pushed RIDNet to SSIM 0.937 / 0.828, highlighting when classical priors still win and where deep residual learning shines.
            </p>
            <ul>
              <li>
                Ran a structured benchmark across classical priors and deep residual/attention models on standard noisy
                datasets.
              </li>
              <li>
                Documented failure modes and tradeoffs (quality vs compute) for practical denoising pipelines.
              </li>
            </ul>
          </article>

          <article class="project">
            <h3>PID Control of Drone with Overhead Vision</h3>
            <p class="meta">Robotics club · Real-time control</p>
            <p>
              Authored a Python SDK around the Pluto drone’s UDP protocol (ARM/BOXARM/SET_ATTITUDE) with interchangeable Xbox/keyboard teleop, then layered calibrated ArUco pose estimation for overhead feedback. Cropping the detection ROI to 300×300 shrank compute by 95.7%, letting PID loops run fast enough to hold course during Inter IIT drone swarm trials.
            </p>
            <ul>
              <li>
                Built a real-time control stack combining teleop, overhead vision pose estimation, and PID stabilization.
              </li>
              <li>
                Optimized the vision loop to keep compute bounded and latency stable during flight.
              </li>
            </ul>
          </article>
        </div>
      </section>
    </main>
  </div>
</body>

</html>
